{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f642e5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OneHotEncoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#encoder\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m encoder = \u001b[43mOneHotEncoder\u001b[49m(drop = \u001b[33m'\u001b[39m\u001b[33mfirst\u001b[39m\u001b[33m'\u001b[39m, sparse_output = \u001b[33m'\u001b[39m\u001b[33mFalse\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m X_train_enc = encoder.fit_transform(X_train)\n\u001b[32m      6\u001b[39m X_test_enc = encoder.transform(X_test)\n",
      "\u001b[31mNameError\u001b[39m: name 'OneHotEncoder' is not defined"
     ]
    }
   ],
   "source": [
    "#ENCODER\n",
    "encoder = OneHotEncoder(drop = 'first', sparse_output = 'False')\n",
    "X_train_enc = encoder.fit_transform(X_train)\n",
    "X_test_enc = encoder.transform(X_test)\n",
    "\n",
    "#create dataframe for encoding\n",
    "feature_names = encoder.get_feature_names_out()\n",
    "X_train_df_enc = pd.DataFrame(data= X_train_enc, columns = feature_names, index = X_train.index)\n",
    "X_test_df_enc = pd.DataFrame(data= X_test_enc, columns = feature_names,index = X_train.index)\n",
    "\n",
    "#remove old cat columns and add new dataframe into it\n",
    "X_train = X_train.drop(columns = cat_col,axis =1)\n",
    "X_train = X_train.concat([X_train,X_train_df_enc], axis =1)\n",
    "#SCALE\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "#BUILDMODEL\n",
    "logreg = LogisticRegression(C =0.0001, random_state =42)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "y_pred_proba = logreg.predict_proba(X_test)[:,1]\n",
    "\n",
    "#EVALUATION\n",
    "classification_report(y_test,y_pred)\n",
    "accuracy_score(y_test,y_pred)\n",
    "f1_score(y_test,y_pred)\n",
    "roc_auc_score(y_test,y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5706b57",
   "metadata": {},
   "source": [
    "#### Label Encoding:\n",
    "- Needed for math algorithms: linear regression, logistics regression, kNN, NN\n",
    "- Remove col after encoding to avoid multicolinearity\n",
    "#### Feature Scaling-Normalization: \n",
    "important because\n",
    "- Make sure features contribute equally, not one dominates the others\n",
    "- Help for faster convergence\n",
    "- Avoid exploding/vanishing problems when calculating with gradients\n",
    "- Need for distance algo: linear regression, kNN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338de827",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA tSNE\n",
    "#DATASET: reduce \n",
    "df =df.drop(columns = 'cell_type', axis =1)\n",
    "X = df.loc[:,df.sum(axis=0) > 10]\n",
    "#Scale\n",
    "X.to_numpy() #before scale, data have to be in numpy arraz instead of df.\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "#PCA\n",
    "pca_50 = PCA(n_components =50)\n",
    "X_pca_50 = pca_50.fit_transform(X_scaled)\n",
    "\n",
    "#plot\n",
    "sns.scatterplot(x = X_pca_50[:,0], y= X_pca_50[:,1], s =10)\n",
    "\n",
    "#tSNE\n",
    "tsne = TSNE(n_components =2, perplexity = 30, random_state = 42)\n",
    "X_tsne = tsne.fit_transform(X_pca_50)\n",
    "sns.scatterplot(x = X_tsne[:,0], y = X_tsne[:,1],s =10, hue =dataset['cell_type'] )\n",
    "\n",
    "#tsne draw with diff seed (random state)\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
    "for i, seed in enumerate([1, 42, 99]):\n",
    "    tsne = TSNE(n_components=2, random_state=seed, perplexity=30, max_iter=500, init=\"random\")\n",
    "    X_tsne = tsne.fit_transform(X_pca_50)\n",
    "    \n",
    "    sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], s=10, ax=axs[i], legend=False)\n",
    "    axs[i].set_title(f\"t-SNE with seed={seed}\")\n",
    "    axs[i].set_xlabel(\"t-SNE 1\")\n",
    "    axs[i].set_ylabel(\"t-SNE 2\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db08122",
   "metadata": {},
   "source": [
    "### PCA: dimension reduction\n",
    "- scale/standardize data\n",
    "- project onto PCA dimension\n",
    "- calc eingen values and eingen vectors, the first few are the most important.\n",
    "- top 2 for visualization, top10 for downstream analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3189201",
   "metadata": {},
   "source": [
    "### Reduce collinearity\n",
    "- df.corr()\n",
    "- Feature selection\n",
    "- Regularization (L1 L2)\n",
    "- Dimension reduction (PCA)\n",
    "- Model agnostic: permutation importance, SHAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f470473",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. CLUSTER ENGINEERING\n",
    "#Feature Engineering\n",
    "rfm.histplot(rfm['Recency'])\n",
    "#remove outliers using IQR\n",
    "Q1 = rfm.Recency.quantile(0.25)\n",
    "Q3 = rfm.Recency.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lowerbound = Q1 - k * IQR\n",
    "upperbound  = Q3 + k * IQR\n",
    "\n",
    "rfm = rfm[ (rfm['Recency'] > lowerbound) & (rfm['Recency']<upperbound)]\n",
    "#cal 2 more times for Monetary and Frequency\n",
    "rfm = rfm[ (rfm['Monetary'] > lowerbound) & (rfm['Monetary']<upperbound)]\n",
    "rfm = rfm[ (rfm['Frequency'] > lowerbound) & (rfm['Frequency']<upperbound)]\n",
    "#Scaling\n",
    "feature_cols = ['Recency', 'Frequency', 'Monetary']\n",
    "scaler = StandardScaler()\n",
    "rfm_scaled = scaler.fit_transform(rfm[feature_cols])\n",
    "#Create df\n",
    "rfm_scaled_df = pd.DataFrame(data=rfm_scaled, columns = feature_cols)\n",
    "#Train\n",
    "#Find optimal k \n",
    "inertia = []\n",
    "k_range = range(2,11)\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters = k, random_state=42)\n",
    "    kmeans.fit(rfm_scaled_df)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "optimal_k = 4\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state = 42)\n",
    "#create a new df col ['Cluster']\n",
    "rfm['Cluster'] = kmeans.fit_predict(rfm_scaled_df)\n",
    "\n",
    "#EVALUATION\n",
    "#groupby Cluster column\n",
    "\n",
    "df_cluster = rfm.groupby('Cluster').agg({\n",
    "    'Recency': 'mean',\n",
    "    'Monetary': 'mean',\n",
    "    'Frequency': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "df_cluster /= df_cluster.max(axis =0)\n",
    "\n",
    "df.heatplot(df_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b997b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HEATMAP\n",
    "sns.heatmap(df.corr())\n",
    "#Train test split\n",
    "#Scale\n",
    "#Lienar regression model without params\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "y_pred = lr.predict(X_test_scaled)\n",
    "#r2 score, mse\n",
    "r2_score = r2_score(y_test,y_pred)\n",
    "mse = mean_square_error(y_test, y_pred)\n",
    "#Plot scatterplot \n",
    "plt.scatter(y_test, y_pred, alpha = 0.6)\n",
    "plt.plot([y_test.min(), y_test.max()],[y_test.min(), y_test.max()], 'r--', label ='Ideal Fit')\n",
    "#Find coefficient\n",
    "feature_names = X_train.columns\n",
    "coefficients = lr.coef_\n",
    "\n",
    "#create df\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Feature':feature_names,\n",
    "    'Coefficient': coefficients\n",
    "})\n",
    "df['Coefficient_Abs'] = np.abs(df.Coefficient)\n",
    "#plot using barplot\n",
    "sns.barplot(x= 'Coefficient' , y ='Feature' , data = df)\n",
    "\n",
    "#LASSO\n",
    "alphas = np.logspace(-4,0,50)\n",
    "model = Lasso(max_iter = 10000)\n",
    "lasso_cv = GridSearchCV(\n",
    "    model,\n",
    "    param_grid = {'alpha':alphas},\n",
    "    cv = 5\n",
    ")\n",
    "#fit lasso\n",
    "lasso_cv.fit(X_train_scaled, y_trian)\n",
    "lasso_cv.best_params_\n",
    "lasso_cv.best_estimators_\n",
    "#lasso df\n",
    "best_model = lasso_cv.best_estimators\n",
    "df_best = pd.DataFrame({\n",
    "    'Feature':feature_names,\n",
    "    'Coefficient':best_model.coef_,\n",
    "    'Abs_Coefficient': np.abs(best_model.coef_)\n",
    "\n",
    "})\n",
    "##barplot\n",
    "sns.barplot(x='Coefficient', y='Feature', data=df_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd00cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permutation importances\n",
    "result = permutation_importances(model, X_test, y_test, scoring ='f1', n_repeats =10, random_state = 4)\n",
    "#df for permu importance and sorted\n",
    "df = pd.DataFrame({\n",
    "    'Feature':feature_names,\n",
    "    'Importance': permu.importances_mean\n",
    "}).sort_values(ascending =True)\n",
    "#visualize barplot\n",
    "sns.barplot(x = 'Importance', y = 'Feature', data = df)\n",
    "#shap\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "shap.summary_plot(shap_values[:,:,1], X_test, plot_type = 'bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28e88c7",
   "metadata": {},
   "source": [
    "PCA\n",
    "- Dimensional reductions by reducing number of features while reserving as much variance as possible.\n",
    "- Standardization -> Find PC -> place points on the most prominent PCs\n",
    "- good to preserve global structure\n",
    "- linear transformation\n",
    "\n",
    "tSNE\n",
    "- Convert distance into probability distribution, try to match 2d dstribution probability to the high dimension one with attractive/repulsive force\n",
    "- preserve local structure\n",
    "- Compute similarities in high dim (compute distance btw every data points using Euclidean, then convert distance to proba using Gaussian distribution) -> place point randomly -> compute similarities in low dim > initiate > compare similarities btw high low using KL divergence >Force (GD)\n",
    "\n",
    "Perplexity: \n",
    "- control how many points in a cluster\n",
    "- balance btw local and global structure\n",
    "- contorl how tSNE define neighborhoods\n",
    "\n",
    "### Entropy: \n",
    "- uncertainty of a dataset. low > better\n",
    "Information Gain: \n",
    "- reduction of entropy\n",
    "Regularization: reduce complexity by adding pernalty.\n",
    "### Decision Tree:\n",
    "- Regularization: PRUNING by reducing complexity bottom up manner, if it decrease the validation error.\n",
    "\n",
    "### Random Forest: \n",
    "- ensemble of many Decision Tree\n",
    "- Key idea: bagging, voting, random subspace method\n",
    "- Bagging: each tree trained on random subset \n",
    "- Voting: each tree vote, majority vote is final prediction\n",
    "- Model complexity: full trees with depth as hyperparams\n",
    "- More robust to overfitting thanks to bagging\n",
    "- Less sensitive by noise or outlier due to averaging process.\n",
    "\n",
    "### Ensemble Learning:\n",
    "- methods of combine multiple learning algorithms to improve in overall improvement\n",
    "\n",
    "### Clustering algorithm:\n",
    "- Quantify similarities btw all data points usig squared Euclidean distance $||x -c||^2$\n",
    "- Pick cluster k >> Place centroid randomly >> Calculate distance btw data points and cluster centroids >> Place points to the cluster >> recalculate centroid >> repeat until max iteration reached.\n",
    "#### Limitation of Random Initializtion:\n",
    "- algorithms get stuck in local optima, good at refining locally but not globally.\n",
    "- Improve: run with different seeds and choose solution with most compact cluster.\n",
    "- Measure cluster compactness: use Inertia or WCSS\n",
    "#### Elbow method\n",
    "- Calculate inertia for all different clusters k .\n",
    "- select elbow points since after adding more clusters does not improve modelling \n",
    "- Not optimal solution (no ground-truth label -unsupervised learning), no universal objective to define best clusters, and number of clusters k is not uniquely determined.\n",
    "\n",
    "#### Gap Statistics\n",
    "- tell us how far awaz our data clustering is from what we expect by chance.\n",
    "- a higher gap means clusters are well seperated, so by maximize the gap we can find cluster config\n",
    "- Compute log(W) where W is WCSS >> compute log_uniform(W) and average >> repeat and calculate Gap(K) >> pick k as the smallest value such that the gap is significantly improved to the largest gap\n",
    "#### Clustering Limitation\n",
    "- Do poorly with outliers\n",
    "- Sensitive to feature scaler\n",
    "- Assume spherical, equal size cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0980d433",
   "metadata": {},
   "source": [
    "#### Build NN from scatch\n",
    "1. Choose model  NN, CNN, RNN, ..\n",
    "2. Build model\n",
    "3. Define cost function\n",
    "4. Select learning rate and optimization\n",
    "5. Apply backprop\n",
    "6. Do hyperparams optimization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
