{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5706b57",
   "metadata": {},
   "source": [
    "#### Label Encoding:\n",
    "- Needed for math algorithms: linear regression, logistics regression, kNN, NN\n",
    "- Remove col after encoding to avoid multicolinearity\n",
    "#### Feature Scaling-Normalization: \n",
    "important because\n",
    "- Make sure features contribute equally, not one dominates the others\n",
    "- Help for faster convergence\n",
    "- Avoid exploding/vanishing problems when calculating with gradients\n",
    "- Need for distance algo: linear regression, kNN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1207a776",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENCODER\n",
    "encoder = OneHotEncoder(drop = 'first', sparse_output = 'False')\n",
    "X_train_enc = encoder.fit_transform(X_train)\n",
    "X_test_enc = encoder.transform(X_test)\n",
    "\n",
    "#create dataframe for encoding\n",
    "feature_names = encoder.get_feature_names_out()\n",
    "X_train_df_enc = pd.DataFrame(data= X_train_enc, columns = feature_names, index = X_train.index)\n",
    "X_test_df_enc = pd.DataFrame(data= X_test_enc, columns = feature_names,index = X_train.index)\n",
    "\n",
    "#remove old cat columns and add new dataframe into it\n",
    "X_train = X_train.drop(columns = cat_col,axis =1)\n",
    "X_train = X_train.concat([X_train,X_train_df_enc], axis =1)\n",
    "#SCALE\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "#BUILDMODEL\n",
    "logreg = LogisticRegression(C =0.0001, random_state =42)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "y_pred_proba = logreg.predict_proba(X_test)[:,1]\n",
    "\n",
    "#EVALUATION\n",
    "classification_report(y_test,y_pred)\n",
    "accuracy_score(y_test,y_pred)\n",
    "f1_score(y_test,y_pred)\n",
    "roc_auc_score(y_test,y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338de827",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA tSNE\n",
    "#DATASET: reduce \n",
    "df =df.drop(columns = 'cell_type', axis =1)\n",
    "X = df.loc[:,df.sum(axis=0) > 10]\n",
    "#Scale\n",
    "X.to_numpy() #before scale, data have to be in numpy arraz instead of df.\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "#PCA\n",
    "pca_50 = PCA(n_components =50)\n",
    "X_pca_50 = pca_50.fit_transform(X_scaled)\n",
    "\n",
    "#plot\n",
    "sns.scatterplot(x = X_pca_50[:,0], y= X_pca_50[:,1], s =10)\n",
    "\n",
    "#tSNE\n",
    "tsne = TSNE(n_components =2, perplexity = 30, random_state = 42)\n",
    "X_tsne = tsne.fit_transform(X_pca_50)\n",
    "sns.scatterplot(x = X_tsne[:,0], y = X_tsne[:,1],s =10, hue =dataset['cell_type'] )\n",
    "\n",
    "#tsne draw with diff seed (random state)\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
    "for i, seed in enumerate([1, 42, 99]):\n",
    "    tsne = TSNE(n_components=2, random_state=seed, perplexity=30, max_iter=500, init=\"random\")\n",
    "    X_tsne = tsne.fit_transform(X_pca_50)\n",
    "    \n",
    "    sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], s=10, ax=axs[i], legend=False)\n",
    "    axs[i].set_title(f\"t-SNE with seed={seed}\")\n",
    "    axs[i].set_xlabel(\"t-SNE 1\")\n",
    "    axs[i].set_ylabel(\"t-SNE 2\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db08122",
   "metadata": {},
   "source": [
    "### PCA: dimension reduction\n",
    "- scale/standardize data\n",
    "- project onto PCA dimension\n",
    "- calc eingen values and eingen vectors, the first few are the most important.\n",
    "- top 2 for visualization, top10 for downstream analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3189201",
   "metadata": {},
   "source": [
    "### Reduce collinearity\n",
    "- df.corr()\n",
    "- Feature selection\n",
    "- Regularization (L1 L2)\n",
    "- Dimension reduction (PCA)\n",
    "- Model agnostic: permutation importance, SHAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f470473",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. CLUSTER ENGINEERING\n",
    "#Feature Engineering\n",
    "rfm.histplot(rfm['Recency'])\n",
    "#remove outliers using IQR\n",
    "Q1 = rfm.Recency.quantile(0.25)\n",
    "Q3 = rfm.Recency.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lowerbound = Q1 - k * IQR\n",
    "upperbound  = Q3 + k * IQR\n",
    "\n",
    "rfm = rfm[ (rfm['Recency'] > lowerbound) & (rfm['Recency']<upperbound)]\n",
    "#cal 2 more times for Monetary and Frequency\n",
    "rfm = rfm[ (rfm['Monetary'] > lowerbound) & (rfm['Monetary']<upperbound)]\n",
    "rfm = rfm[ (rfm['Frequency'] > lowerbound) & (rfm['Frequency']<upperbound)]\n",
    "#Scaling\n",
    "feature_cols = ['Recency', 'Frequency', 'Monetary']\n",
    "scaler = StandardScaler()\n",
    "rfm_scaled = scaler.fit_transform(rfm[feature_cols])\n",
    "#Create df\n",
    "rfm_scaled_df = pd.DataFrame(data=rfm_scaled, columns = feature_cols)\n",
    "#Train\n",
    "#Find optimal k \n",
    "inertia = []\n",
    "k_range = range(2,11)\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters = k, random_state=42)\n",
    "    kmeans.fit(rfm_scaled_df)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "optimal_k = 4\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state = 42)\n",
    "#create a new df col ['Cluster']\n",
    "rfm['Cluster'] = kmeans.fit_predict(rfm_scaled_df)\n",
    "\n",
    "#EVALUATION\n",
    "#groupby Cluster column\n",
    "\n",
    "df_cluster = rfm.groupby('Cluster').agg({\n",
    "    'Recency': 'mean',\n",
    "    'Monetary': 'mean',\n",
    "    'Frequency': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "df_cluster /= df_cluster.max(axis =0)\n",
    "\n",
    "df.heatplot(df_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b997b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HEATMAP\n",
    "sns.heatmap(df.corr())\n",
    "#Train test split\n",
    "#Scale\n",
    "#Lienar regression model without params\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "y_pred = lr.predict(X_test_scaled)\n",
    "#r2 score, mse\n",
    "r2_score = r2_score(y_test,y_pred)\n",
    "mse = mean_square_error(y_test, y_pred)\n",
    "#Plot scatterplot \n",
    "plt.scatter(y_test, y_pred, alpha = 0.6)\n",
    "plt.plot([y_test.min(), y_test.max()],[y_test.min(), y_test.max()], 'r--', label ='Ideal Fit')\n",
    "#Find coefficient\n",
    "feature_names = X_train.columns\n",
    "coefficients = lr.coef_\n",
    "\n",
    "#create df\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Feature':feature_names,\n",
    "    'Coefficient': coefficients\n",
    "})\n",
    "df['Coefficient_Abs'] = np.abs(df.Coefficient)\n",
    "#plot using barplot\n",
    "sns.barplot(x= 'Coefficient' , y ='Feature' , data = df)\n",
    "\n",
    "#LASSO\n",
    "alphas = np.logspace(-4,0,50)\n",
    "model = Lasso(max_iter = 10000)\n",
    "lasso_cv = GridSearchCV(\n",
    "    model,\n",
    "    param_grid = {'alpha':alphas},\n",
    "    cv = 5\n",
    ")\n",
    "#fit lasso\n",
    "lasso_cv.fit(X_train_scaled, y_trian)\n",
    "lasso_cv.best_params_\n",
    "lasso_cv.best_estimators_\n",
    "#lasso df\n",
    "best_model = lasso_cv.best_estimators\n",
    "df_best = pd.DataFrame({\n",
    "    'Feature':feature_names,\n",
    "    'Coefficient':best_model.coef_,\n",
    "    'Abs_Coefficient': np.abs(best_model.coef_)\n",
    "\n",
    "})\n",
    "##barplot\n",
    "sns.barplot(x='Coefficient', y='Feature', data=df_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd00cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permutation importances\n",
    "result = permutation_importances(model, X_test, y_test, scoring ='f1', n_repeats =10, random_state = 4)\n",
    "#df for permu importance and sorted\n",
    "df = pd.DataFrame({\n",
    "    'Feature':feature_names,\n",
    "    'Importance': permu.importances_mean\n",
    "}).sort_values(ascending =True)\n",
    "#visualize barplot\n",
    "sns.barplot(x = 'Importance', y = 'Feature', data = df)\n",
    "#shap\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "shap.summary_plot(shap_values[:,:,1], X_test, plot_type = 'bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852e7d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REBUILD WITH OPTIMIZER'S learning_rate = 0.05\n",
    "\n",
    "#1: Build model Sequential\n",
    "import keras\n",
    "nn2 = Sequential([\n",
    "    keras.Input(shape = (32,32,3)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation ='relu'),\n",
    "    Dense(10, activation = 'softmax')\n",
    "])\n",
    "nn2.summary()\n",
    "\n",
    "#2: Compile model\n",
    "from keras.optimizers import SGD\n",
    "nn2.compile(\n",
    "    optimizer = SGD(learning_rate = 0.05),\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "nn2.summary()\n",
    "\n",
    "#3: Train model\n",
    "history2 = nn2.fit(\n",
    "    X_train, y_train_cat,\n",
    "    validation_data = (X_test, y_test_cat),\n",
    "    verbose = 1,\n",
    "    epochs = 50,\n",
    "    batch_size = 64\n",
    ")\n",
    "\n",
    "#4: Evaluate model\n",
    "nn2.evaluate(X_test, y_test_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db86b958",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = DecisionTreeClassifier(criterion = \"entropy\", min_samples_leaf = 3)\n",
    "# Lots of parameters: criterion = \"gini\" / \"entropy\";\n",
    "# max_depth;\n",
    "# min_impurity_split;\n",
    "clf.fit(X, y) # It can only handle numerical attributes!\n",
    "# Categorical attributes need to be encoded, see LabelEncoder and OneHotEncoder\n",
    "clf.predict([x]) # Predict class for x\n",
    "clf.feature_importances_ # Importance of each feature\n",
    "clf.tree_ # The underlying tree object\n",
    "clf = RandomForestClassifier(n_estimators = 20) # Random Forest with 20 trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756036fc",
   "metadata": {},
   "source": [
    "# DM\n",
    "### Domain Knowledge\n",
    "- Reduce risk of Type 2 erro : increase sample size\n",
    "- Class imbalance: in small dataset, model tend to bias towards majority class.\n",
    "- Handle imbalanced: down sample or up sampling.\n",
    "- Ensure accuracy, generalization and computational feasibility from the start.\n",
    "- Binning (bucketting): turn numerical to categorical data.\n",
    "- Feature scaling: required for distance based model (KNN, clustering)---scale so that features contribute equally, not 1 dominates others---help GD faster convergence---avoid exploding/vanishing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28e88c7",
   "metadata": {},
   "source": [
    "### Dimensional Reduction\n",
    "\n",
    "PCA\n",
    "- Dimensional reductions by reducing number of features while reserving as much variance as possible.\n",
    "- Standardization -> Find PC -> place points on the most prominent PCs\n",
    "- good to preserve global structure\n",
    "- linear transformation\n",
    "- Centering ensures true shape and spread of data.\n",
    "- Measure the variance called 'eigen value'\n",
    "\n",
    "tSNE\n",
    "- Convert distance into probability distribution, try to match 2d dstribution probability to the high dimension one with attractive/repulsive force\n",
    "- preserve local structure\n",
    "- Compute similarities in high dim (compute distance btw every data points using Euclidean, then convert distance to proba using Gaussian distribution) -> place point randomly -> compute similarities in low dim > initiate > compare similarities btw high low using KL divergence >Force (GD)\n",
    "\n",
    "Perplexity: \n",
    "- control how many points in a cluster\n",
    "- balance btw local and global structure\n",
    "- contorl how tSNE define neighborhoods\n",
    "### Validation method\n",
    "- Precision/recall/accu/specificity\n",
    "- F1 score = 2 Precision * Recall / (Precision + Recall)\n",
    "- ROC AUC\n",
    "### Training:\n",
    "- fit model >> compute Loss function >> update weights and bias >> predict >> again\n",
    "### Hyperparams vs model params\n",
    "- Tuned using: validation method (learning rate, n_clusters) & optimization algorithms (for weight/bias)\n",
    "### Entropy: \n",
    "- uncertainty of a dataset. low > better\n",
    "Information Gain: \n",
    "- reduction of entropy\n",
    "Regularization: reduce complexity by adding pernalty.\n",
    "### Decision Tree:\n",
    "- Regularization: PRUNING by reducing complexity bottom up manner, if it decrease the validation error.\n",
    "\n",
    "### Random Forest: \n",
    "- ensemble of many Decision Tree\n",
    "- Key idea: bagging, voting, random subspace method\n",
    "- Bagging: each tree trained on random subset \n",
    "- Voting: each tree vote, majority vote is final prediction\n",
    "- Model complexity: full trees with depth as hyperparams\n",
    "- More robust to overfitting thanks to bagging\n",
    "- Less sensitive by noise or outlier due to averaging process.\n",
    "\n",
    "### Ensemble Learning:\n",
    "- methods of combine multiple learning algorithms to improve in overall improvement\n",
    "- Bagging: train multiple models on random subset of dataset\n",
    "- Random Subspace Method: multiple models on random subset of features\n",
    "- Boosting: train iteratively, while making current models learn previous models mistake by increasing weight of missclassified samples.\n",
    "### Clustering algorithm:\n",
    "- Quantify similarities btw all data points usig squared Euclidean distance $||x -c||^2$\n",
    "- Pick cluster k >> Place centroid randomly >> Calculate distance btw data points and cluster centroids >> Place points to the cluster >> recalculate centroid >> repeat until max iteration reached.\n",
    "#### Limitation of Random Initializtion:\n",
    "- algorithms get stuck in local optima, good at refining locally but not globally.\n",
    "- Improve: run with different seeds and choose solution with most compact cluster.\n",
    "- Measure cluster compactness: use Inertia or WCSS\n",
    "#### Elbow method\n",
    "- Calculate inertia for all different clusters k .\n",
    "- select elbow points since after adding more clusters does not improve modelling \n",
    "- Not optimal solution (no ground-truth label -unsupervised learning), no universal objective to define best clusters, and number of clusters k is not uniquely determined.\n",
    "\n",
    "#### Gap Statistics\n",
    "- tell us how far awaz our data clustering is from what we expect by chance.\n",
    "- a higher gap means clusters are well seperated, so by maximize the gap we can find cluster config\n",
    "- Compute log(W) where W is WCSS >> compute log_uniform(W) and average >> repeat and calculate Gap(K) >> pick k as the smallest value such that the gap is significantly improved to the largest gap\n",
    "#### Clustering Limitation\n",
    "- Do poorly with outliers\n",
    "- Sensitive to feature scaler\n",
    "- Assume spherical, equal size cluster\n",
    "#### Clutering purpose\n",
    "- for explanability, not for predicting outcomes\n",
    "### DL special\n",
    "- Recognize patterns by themselves using self-estimated features\n",
    "### Fit the model\n",
    "- Is to find best parameters that has acceptable error margin but still reliable.\n",
    "\n",
    "#### Why we go deep in NN ?\n",
    "- To learn the hierachical features that helps the prediction\n",
    "#### Choose right network size\n",
    "- Network size is the number of layers and neuron in each layer.\n",
    "- just a hyperparams to tune, can consider other method like Early stopping, regularization.\n",
    "#### Training difficulty\n",
    "- Exploding/ vanishing gradients when using activation functions like sigmoid or tanh\n",
    "#### Optimization tricks\n",
    "- Weight initialization\n",
    "- Learning rate decay: during training\n",
    "- SGD, minibatch GD\n",
    "- Gradient clipping\n",
    "#### Build NN from scatch\n",
    "1. Choose model  NN, CNN, RNN, ..\n",
    "2. Build model\n",
    "3. Define cost function\n",
    "4. Select learning rate and optimization\n",
    "5. Apply backprop\n",
    "6. Do hyperparams optimization\n",
    "\n",
    "#### RAG\n",
    "- Embedding method: turn text chunks into vectors for semantic search\n",
    "- Use sentence embedding\n",
    "- Vector DB: store and retrieve text by vectorrs\n",
    "- Most similar chunks: distance metric (Cosine, Dot Product, Euclidean, Manhattan)\n",
    "- Contextual Answer: retrieved chunks + questions for LLM to provide Contextual answer\n",
    "#### Application of Explanability: Feature Importance Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4cb6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.compile(\n",
    "    optimizer = SGD(learning_rate = 0.05),\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "nn.summary()\n",
    "nn.fit(\n",
    "    X_train, y_train_cat,\n",
    "    validation_data = (X_test, y_test_cat),\n",
    "    verbose = 1,\n",
    "    epochs = 50,\n",
    "    batch_size = 64\n",
    ")\n",
    "nn.evaluate(X_test, y_test_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4153c04a",
   "metadata": {},
   "source": [
    "\n",
    "# Data Mining / Data Science Project\n",
    "\n",
    "A structured collection of **machine learning, data mining, and deep learning exercises** implemented in Python using Jupyter Notebooks, practised as a 3 months spanned project. This repository demonstrates **end-to-end ML pipelines**, from data preprocessing and classical models to **CNNs and explainable AI**.\n",
    "\n",
    "# Introduction & Goals\n",
    "\n",
    "- Throughout the projects, those standard workflow are discussed and practised that including:\n",
    "- Problem understanding.\n",
    "- Data exploration & preprocessing.\n",
    "- Feature engineering. \n",
    "- Model sectiontion & training.\n",
    "- Validation & parameter tuning.\n",
    "- Testing & communication results.\n",
    "- Deployment & monitoring. \n",
    "\n",
    "\n",
    "# Contents\n",
    "\n",
    "Each notebook focuses on **both implementation and conceptual understanding**.\n",
    "\n",
    "## Repository Structure\n",
    "```text\n",
    "â”œâ”€â”€ Exercise_2_Pandas_Introduction.ipynb\n",
    "â”œâ”€â”€ Exercise_3_4_DataMiningPipeline_LR.ipynb\n",
    "â”œâ”€â”€ Exercise_5_PCA_tSNE.ipynb\n",
    "â”œâ”€â”€ Exercise_6_ClassificationPipeline_RDF.ipynb\n",
    "â”œâ”€â”€ Exercise_7_clustering.ipynb\n",
    "â”œâ”€â”€ Exercise_8_HeatingLoadRegression.ipynb\n",
    "â”œâ”€â”€ Exercise_9_10_NeuralNetWorks_fromLR_to_NN.ipynb\n",
    "â”œâ”€â”€ Exercise_10_plus_ImageRecognition_CNN.ipynb\n",
    "â””â”€â”€ Exercise_11_Explainability_Analysis.ipynb\n",
    "```\n",
    "\n",
    "# Exercise Highlights\n",
    "## ðŸ“— Exercise 2 â€“ Pandas Introduction\n",
    "**File:** [`Exercise_2_Pandas_Introduction.ipynb`](Exercise_2_Pandas_Introduction.ipynb)\n",
    "\n",
    "\n",
    "### Overview\n",
    "Introduction to **Pandas** for tabular data manipulation and exploration.\n",
    "\n",
    "### What I have learned\n",
    "- How to create and manipulate `Series` and `DataFrame`\n",
    "- The difference between `loc` and `iloc` indexing\n",
    "- How to filter data using logical conditions\n",
    "- How to handle missing values and basic data cleaning\n",
    "- Why Pandas is the foundation of all data science and ML workflows\n",
    "\n",
    "## ðŸ“— Exercise 3 & 4 â€“ Data Mining Pipeline & Logistic Regression\n",
    "\n",
    "**File:** [`Exercise_3_4_DataMiningPipeline_LR.ipynb`](Exercise_3_4_DataMiningPipeline_LR.ipynb)\n",
    "\n",
    "\n",
    "### Overview\n",
    "Implementation of a complete machine learning pipeline using **Logistic Regression**.\n",
    "\n",
    "### What I have learned\n",
    "- How to structure an end-to-end machine learning pipeline\n",
    "- The importance of proper train/test splitting\n",
    "- How Logistic Regression performs binary classification\n",
    "- The difference between class prediction and probability prediction\n",
    "- Why metrics like **ROC-AUC** are preferred for imbalanced datasets\n",
    "- How data preparation impacts model performance more than the model itself\n",
    "\n",
    "## ðŸ“— Exercise 5 â€“ PCA & t-SNE\n",
    "\n",
    "**File:** [`Exercise_5_PCA_tSNE.ipynb`](Exercise_5_PCA_tSNE.ipynb)\n",
    "\n",
    "### Overview\n",
    "Dimensionality reduction techniques applied to high-dimensional biological data.\n",
    "\n",
    "### What I have learned\n",
    "- Why standardization is mandatory before applying PCA\n",
    "- How PCA reduces dimensionality while preserving variance\n",
    "- How to interpret PCA scatter plots\n",
    "- Why t-SNE is useful for visualization but not for modeling\n",
    "- The conceptual difference between linear (PCA) and non-linear (t-SNE) methods\n",
    "\n",
    "## ðŸ“— Exercise 6 â€“ Classification Pipeline with Random Forest\n",
    "\n",
    "**File:** [`Exercise_6_ClassificationPipeline_RDF.ipynb`](Exercise_6_ClassificationPipeline_RDF.ipynb)\n",
    "\n",
    "### Overview\n",
    "Supervised classification using **Random Forest** and ensemble learning.\n",
    "\n",
    "### What I have learned\n",
    "- How ensemble methods improve model robustness\n",
    "- Why Random Forests are less prone to overfitting\n",
    "- That tree-based models do not require feature scaling\n",
    "- How feature importance is computed in tree-based models\n",
    "- The trade-off between model performance and interpretability\n",
    "\n",
    "## ðŸ“— Exercise 7 â€“ Clustering\n",
    "\n",
    "**File:** [`Exercise_7_clustering.ipynb`](Exercise_7_clustering.ipynb)\n",
    "\n",
    "### Overview\n",
    "Unsupervised learning using **K-Means clustering**.\n",
    "\n",
    "### What I have learned\n",
    "- The difference between supervised and unsupervised learning\n",
    "- Why scaling is critical for distance-based clustering algorithms\n",
    "- How the Elbow Method helps estimate the optimal number of clusters\n",
    "- What inertia measures and its limitations\n",
    "- That clustering results are heuristic and context-dependent\n",
    "\n",
    "## ðŸ“— Exercise 8 â€“ Heating Load Regression\n",
    "\n",
    "**File:** [`Exercise_8_HeatingLoadRegression.ipynb`](Exercise_8_HeatingLoadRegression.ipynb)\n",
    "\n",
    "### Overview\n",
    "Regression modeling for predicting heating load based on building features.\n",
    "\n",
    "### What I have learned\n",
    "- How linear regression models continuous target variables\n",
    "- How to analyze correlations between features and target\n",
    "- What RÂ² score represents in regression tasks\n",
    "- Why multicollinearity affects interpretability\n",
    "- How MSE penalizes large prediction errors\n",
    "\n",
    "## ðŸ“— Exercise 9 & 10 â€“ From Logistic Regression to Neural Networks\n",
    "\n",
    "**File:** [`Exercise_9_10_NeuralNetWorks_fromLR_to_NN.ipynb`](Exercise_9_10_NeuralNetWorks_fromLR_to_NN.ipynb)\n",
    "\n",
    "### Overview\n",
    "Transition from traditional machine learning to **neural networks**.\n",
    "\n",
    "### What I have learned\n",
    "- Logistic Regression can be seen as a single-layer neural network\n",
    "- Why one-hot encoding is required for multi-class classification\n",
    "- How fully connected neural networks process image data\n",
    "- The role of activation functions and loss functions\n",
    "- The conceptual difference between forward propagation and backpropagation\n",
    "\n",
    "## ðŸ“— Exercise 10+ â€“ Image Recognition with CNN\n",
    "\n",
    "**File:** [`Exercise_10_plus_ImageRecognition_CNN.ipynb`](Exercise_10_plus_ImageRecognition_CNN.ipynb)\n",
    "\n",
    "### Overview\n",
    "Image classification using **Convolutional Neural Networks (CNNs)**.\n",
    "\n",
    "### What I have learned\n",
    "- How convolutional layers extract spatial features\n",
    "- Why CNNs outperform fully connected networks on image data\n",
    "- How pooling layers reduce dimensionality and overfitting\n",
    "- The concept of parameter sharing in CNNs\n",
    "- How CNNs learn hierarchical features from edges to objects\n",
    "\n",
    "## ðŸ“— Exercise 11 â€“ Explainability Analysis\n",
    "\n",
    "**File:** [`Exercise_11_Explainability_Analysis.ipynb`](Exercise_11_Explainability_Analysis.ipynb)\n",
    "\n",
    "### Overview\n",
    "Explainable AI techniques applied to classification models.\n",
    "\n",
    "### What I have learned\n",
    "- Why model accuracy alone is not sufficient\n",
    "- The importance of explainability in real-world ML systems\n",
    "- The difference between global and local explanations\n",
    "- How different feature importance methods yield different insights\n",
    "- Why trust and transparency are essential for deploying ML models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccbfc68",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
