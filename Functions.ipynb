{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31b64820",
   "metadata": {},
   "source": [
    "### 1. Softmax Function\n",
    "The softmax function is defined as:\n",
    "\n",
    "$$\n",
    "\\sigma(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}\n",
    "$$\n",
    "\n",
    "-About how neural network outputs probabilities for each class. \\\n",
    "-Comes at the end of NN, in the output layer.\n",
    "\n",
    "#### Meaning of terms:\n",
    "- $z_i$: the *i*-th input score (logit)\n",
    "- $K$: total number of classes or outputs\n",
    "- $e^{z_i}$: exponential of the *i*-th score\n",
    "- $\\sum_{j=1}^{K} e^{z_j}$: normalization term to ensure all outputs sum to 1\n",
    "- $\\sigma(z_i)$: probability of class *i*\n",
    "\n",
    "Thus, Softmax ensures:\n",
    "$$\n",
    "\\sum_{i=1}^{K} \\sigma(z_i) = 1\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55c9dc7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: [2.  1.  0.1]\n",
      "Exp_val: [7.3890561  2.71828183 1.10517092]\n",
      "Sum of exp_val  11.212508845465344\n",
      "Softmax probabilities: [0.65900114 0.24243297 0.09856589]\n",
      "Sum of probabilities: 1.0\n"
     ]
    }
   ],
   "source": [
    "#EXAMPLE OF SOFTMAX FUNCTION\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Raw scores (logits) output by a model\n",
    "logits = np.array([2.0, 1.0, 0.1])\n",
    "\n",
    "# Compute softmax manually\n",
    "exp_vals = np.exp(logits)\n",
    "softmax_probs = exp_vals / np.sum(exp_vals)\n",
    "\n",
    "print(\"Logits:\", logits)\n",
    "print(\"Exp_val:\", exp_vals)\n",
    "print(\"Sum of exp_val \", np.sum(exp_vals))\n",
    "print(\"Softmax probabilities:\", softmax_probs)\n",
    "print(\"Sum of probabilities:\", np.sum(softmax_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1fb310",
   "metadata": {},
   "source": [
    "### 2. One hot encoding\n",
    "-This converts integer class labels → vectors of 0s and 1s.\n",
    "-How we encode the target classes (correct answer).\n",
    "-Comes before training, as preprocessing for labels (y_train).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cca45d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original labels:\n",
      " [0 2 1 3 2]\n",
      "One-hot encoded:\n",
      " [[1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#EXAMPLE OF ONE-HOT CODING\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "# Suppose we have 5 samples with class labels from 0 to 3\n",
    "y = np.array([0, 2, 1, 3, 2])\n",
    "\n",
    "# One-hot encode\n",
    "y_onehot = to_categorical(y, num_classes=4)\n",
    "\n",
    "print(\"Original labels:\\n\", y)\n",
    "print(\"One-hot encoded:\\n\", y_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1948bb8c",
   "metadata": {},
   "source": [
    "### ONE HOT ENCODING & SOFTMAX WORK TOGETHER\n",
    "✅ The smaller the loss, the closer the predicted probability is to the true class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a94a770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy loss: 0.3566749425101611\n"
     ]
    }
   ],
   "source": [
    "# True label (one-hot encoded)\n",
    "y_true = np.array([0, 1, 0])   # class 1 is correct\n",
    "\n",
    "# Model prediction (softmax output)\n",
    "y_pred = np.array([0.1, 0.7, 0.2])\n",
    "\n",
    "# Cross-entropy loss (manual calculation)\n",
    "loss = -np.sum(y_true * np.log(y_pred + 1e-9))  # small epsilon to avoid log(0)\n",
    "print(\"Cross-entropy loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79eb4b2b",
   "metadata": {},
   "source": [
    "### EQUATION\n",
    "\n",
    "-mean square  \n",
    "-entropy  \n",
    "-(binary) cross entropy  \n",
    "-softmax  \n",
    "-sigmoid  \n",
    "-scaling (mix max mean median)  \n",
    "-GDecent w = w -a * d(J)/dw  \n",
    "SGD  \n",
    "minibatch SGD  \n",
    "softmax with temperature  \n",
    "R2 score for linear regression  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5329d0c2",
   "metadata": {},
   "source": [
    "### 3. Entropy\n",
    "Entropy measure the degree of randomness in data. \n",
    "For a set of sample X with k classes:\n",
    "$$\n",
    "entropy(X) = - \\sum_{i=1}^{k} p_i \\log_2(p_i)\n",
    "$$\n",
    "where ${p_i}$ is the proportion of elements of class i.\n",
    "\n",
    "Lower entropy implies greater predictability.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5f9360",
   "metadata": {},
   "source": [
    "### 4. Information Gain\n",
    "The information gain of an attribute a is the expected reduction in entropy due to splitting on values of a:\n",
    "\n",
    "$$\n",
    "gain(X, a) = entropy(X) \\;-\\; \n",
    "\\sum_{v \\in values(a)} \n",
    "\\frac{|X_v|}{|X|} \\; entropy(X_v)\n",
    "$$\n",
    "\n",
    "where $X_v$ is the subset of $X$ for which ${a = v}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e96699",
   "metadata": {},
   "source": [
    "### 5. Euclidean distance\n",
    "$$\n",
    "{||x-c||}^2 = ({x_1} - {c_1})^2 + ({x_2} - {c_2})^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bba08e",
   "metadata": {},
   "source": [
    "### 6. Linear Regression\n",
    "\n",
    "$$\n",
    "y = b + \\sum_{i} {w_i}{x_i}\n",
    "$$\n",
    "\n",
    "Where: \\\n",
    "y: output \\\n",
    "${x_i}$: $i^{th}$ input \\\n",
    "${w_i}$: weight on $i^{th}$ input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3487678e",
   "metadata": {},
   "source": [
    "### 7. Lost Function \n",
    "\n",
    "$$\n",
    "(y^{<i>}- \\hat{y}^{<i>})^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d66cb5c",
   "metadata": {},
   "source": [
    "### 8. Cost Funtion (all sample) - MSE Mean Square Error\n",
    "$$\n",
    "MSE = \\frac{1}{2M} \\sum_{i=1}^{M} (y^{<i>} - \\hat{y}^{<i>})^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc5ac55",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6aa4245a",
   "metadata": {},
   "source": [
    "### 9. Sigmoid Activation Function\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^\\ (-z)}\n",
    "$$\n",
    "with $z = w^Tx +b$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3be873",
   "metadata": {},
   "source": [
    "### 10. Gradient Descent\n",
    "- GD is the derivative of a function with respect to its input variable and it shows Direction and Magnitude of the Steepest Ascent.\n",
    "$$\n",
    "\\theta_j \\leftarrow \\theta_j\n",
    "- \\alpha \\, \\frac{\\partial}{\\partial \\theta_j} J(\\theta)\n",
    "$$\n",
    "- Weight and bias update: \n",
    "$$\n",
    "w = w\n",
    "- \\alpha \\, \\frac{\\partial}{\\partial w} J(w, b)\n",
    "$$\n",
    "$$\n",
    "b = b\n",
    "- \\alpha \\, \\frac{\\partial}{\\partial b} J(w, b)\n",
    "$$\n",
    "Where:\n",
    "- w and b: weight and bias\n",
    "- $\\alpha$: learning rate\n",
    "- $J(\\theta) = J(w,b)$ = MSE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8d22c2",
   "metadata": {},
   "source": [
    "### 11. Binary Cross-Entropy\n",
    "$$\n",
    "J(\\omega, b)\n",
    "= - \\frac{1}{M} \\sum_{i=1}^{M}\n",
    "\\left[\n",
    "y^{(i)} \\log \\hat{y}^{(i)}\n",
    "+ (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)})\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e7f850",
   "metadata": {},
   "source": [
    "### 12. The **R²(Coefficient of Determination)** indicates how much variance in the target is explained by the model.\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{SS_{\\text{res}}}{SS_{\\text{tot}}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $SS_{\\text{res}} = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$\n",
    "  (Residual Sum of Squares: error between true and predicted values)\n",
    "\n",
    "* $SS_{\\text{tot}} = \\sum_{i=1}^{n}(y_i - \\bar{y})^2$\n",
    "  (Total Sum of Squares: error between true values and their mean)\n",
    "\n",
    "* $y_i$: true value\n",
    "\n",
    "* $\\hat{y}_i$: predicted value\n",
    "\n",
    "* $\\bar{y}$: mean of all $y_i$\n",
    "\n",
    "Interpretation\n",
    "* $R^2 = 1$: perfect prediction\n",
    "* $R^2 = 0$: model predicts no better than mean\n",
    "* $R^2 < 0$: model is worse than just predicting the mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7710fe00",
   "metadata": {},
   "source": [
    "### 13. Lasso Regression (L1)\n",
    "Lasso(L1 Regularization) minimizes the sum of squared errors **plus** a penalty proportional to the **absolute values of the coefficients**. This L1 penalty encourages **sparsity**, meaning it pushes some coefficients to **zero**, effectively performing feature selection.\n",
    "\n",
    "The loss function of Lasso regression is:\n",
    "\n",
    "$$\\mathcal{L}(\\beta) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( y_i - \\hat{y}_i \\right)^2 + \\alpha \\sum_{j=1}^{p} |w_j|$$\n",
    "\n",
    "Where:\n",
    "* $m$: number of samples\n",
    "* $y_i$: actual target value for sample $i$\n",
    "* $\\hat{y}_i = X_i \\cdot \\beta$: predicted value for sample $i$\n",
    "* $w_j$: the $j$-th coefficient of the model\n",
    "* $\\alpha \\geq 0$: regularization strength\n",
    "* The **first term** is the **mean squared error (MSE)**\n",
    "* The **second term** is the **L1 penalty** (sum of absolute values of the coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b89566f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
