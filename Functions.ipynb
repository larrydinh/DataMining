{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31b64820",
   "metadata": {},
   "source": [
    "### 1. Softmax Function\n",
    "The softmax function is defined as:\n",
    "\n",
    "$$\n",
    "\\sigma(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}\n",
    "$$\n",
    "\n",
    "-About how neural network outputs probabilities for each class. \\\n",
    "-Comes at the end of NN, in the output layer.\n",
    "\n",
    "#### Meaning of terms:\n",
    "- $z_i$: the *i*-th input score (logit)\n",
    "- $K$: total number of classes or outputs\n",
    "- $e^{z_i}$: exponential of the *i*-th score\n",
    "- $\\sum_{j=1}^{K} e^{z_j}$: normalization term to ensure all outputs sum to 1\n",
    "- $\\sigma(z_i)$: probability of class *i*\n",
    "\n",
    "Thus, Softmax ensures:\n",
    "$$\n",
    "\\sum_{i=1}^{K} \\sigma(z_i) = 1\n",
    "$$\n",
    "\n",
    "### 1.2. Temperature_Based Sampling\n",
    "Used to ensure stochasticity of LLM models\n",
    "$$\n",
    "p(x_i) = \\frac{e^\\frac{z_i}{T}} \n",
    "\n",
    "{\\sum_{j=1}^{K}e^\\frac{z_j}{T}}\n",
    "$$\n",
    "\n",
    "T = 0: default value \\\n",
    "T < 1: less creative (more deterministic) \\\n",
    "T > 1: more creative (more randomness) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55c9dc7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: [2.  1.  0.1]\n",
      "Exp_val: [7.3890561  2.71828183 1.10517092]\n",
      "Sum of exp_val  11.212508845465344\n",
      "Softmax probabilities: [0.65900114 0.24243297 0.09856589]\n",
      "Sum of probabilities: 1.0\n"
     ]
    }
   ],
   "source": [
    "#EXAMPLE OF SOFTMAX FUNCTION\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Raw scores (logits) output by a model\n",
    "logits = np.array([2.0, 1.0, 0.1])\n",
    "\n",
    "# Compute softmax manually\n",
    "exp_vals = np.exp(logits)\n",
    "softmax_probs = exp_vals / np.sum(exp_vals)\n",
    "\n",
    "print(\"Logits:\", logits)\n",
    "print(\"Exp_val:\", exp_vals)\n",
    "print(\"Sum of exp_val \", np.sum(exp_vals))\n",
    "print(\"Softmax probabilities:\", softmax_probs)\n",
    "print(\"Sum of probabilities:\", np.sum(softmax_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1fb310",
   "metadata": {},
   "source": [
    "### 2. One hot encoding\n",
    "-This converts integer class labels → vectors of 0s and 1s.\n",
    "-How we encode the target classes (correct answer).\n",
    "-Comes before training, as preprocessing for labels (y_train).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cca45d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original labels:\n",
      " [0 2 1 3 2]\n",
      "One-hot encoded:\n",
      " [[1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#EXAMPLE OF ONE-HOT CODING\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "# Suppose we have 5 samples with class labels from 0 to 3\n",
    "y = np.array([0, 2, 1, 3, 2])\n",
    "\n",
    "# One-hot encode\n",
    "y_onehot = to_categorical(y, num_classes=4)\n",
    "\n",
    "print(\"Original labels:\\n\", y)\n",
    "print(\"One-hot encoded:\\n\", y_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1948bb8c",
   "metadata": {},
   "source": [
    "### ONE HOT ENCODING & SOFTMAX WORK TOGETHER\n",
    "✅ The smaller the loss, the closer the predicted probability is to the true class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a94a770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy loss: 0.3566749425101611\n"
     ]
    }
   ],
   "source": [
    "# True label (one-hot encoded)\n",
    "y_true = np.array([0, 1, 0])   # class 1 is correct\n",
    "\n",
    "# Model prediction (softmax output)\n",
    "y_pred = np.array([0.1, 0.7, 0.2])\n",
    "\n",
    "# Cross-entropy loss (manual calculation)\n",
    "loss = -np.sum(y_true * np.log(y_pred + 1e-9))  # small epsilon to avoid log(0)\n",
    "print(\"Cross-entropy loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79eb4b2b",
   "metadata": {},
   "source": [
    "### EQUATION\n",
    "\n",
    "-mean square  \n",
    "-entropy  \n",
    "-(binary) cross entropy  \n",
    "-softmax  \n",
    "-sigmoid  \n",
    "-scaling (mix max mean median)  \n",
    "-GDecent w = w -a * d(J)/dw  \n",
    "-SGD  \n",
    "-minibatch SGD  \n",
    "-softmax with temperature  \n",
    "-R2 score for linear regression  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c761f019",
   "metadata": {},
   "source": [
    "### 3. Mean Squared Error (MSE) - Cost Functions for all data points\n",
    "\n",
    "$$\n",
    "\\mathrm{MSE} = \\frac{1}{2n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "**Where**\n",
    "- $n$: number of samples  \n",
    "- $y_i$: true value of sample \\(i\\)  \n",
    "- $\\hat{y}_i$: predicted value of sample \\(i\\)\n",
    "\n",
    "**Intuition**\n",
    "- Measures average squared prediction error  \n",
    "- Large errors are penalized more strongly  \n",
    "- Common loss for regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5329d0c2",
   "metadata": {},
   "source": [
    "### 4. Entropy\n",
    "Entropy measure the degree of randomness in data. \n",
    "For a set of sample X with k classes:\n",
    "$$\n",
    "entropy(X) = - \\sum_{i=1}^{k} p_i \\log_2(p_i)\n",
    "$$\n",
    "where ${p_i}$ is probability of the outcome,\n",
    "X: random variable.\n",
    "\n",
    "Lower entropy implies greater predictability.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8d22c2",
   "metadata": {},
   "source": [
    "### 5. Binary Cross Entropy\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{BCE}} = -\\frac{1}{n}\\sum_{i=1}^{n}\n",
    "\\left[y_i \\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i)\\right]\n",
    "$$\n",
    "\n",
    "**Where**\n",
    "- $y_i \\in \\{0,1\\}$: true label  \n",
    "- $\\hat{y}_i \\in [0,1]$: predicted probability  \n",
    "- $n$: number of samples\n",
    "\n",
    "**Intuition**\n",
    "- Strongly penalizes confident wrong predictions  \n",
    "- Standard loss for binary classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5f9360",
   "metadata": {},
   "source": [
    "### 6. Information Gain\n",
    "The information gain of an attribute a is the expected reduction in entropy due to splitting on values of a:\n",
    "\n",
    "$$\n",
    "gain(X, a) = entropy(X) \\;-\\; \n",
    "\\sum_{v \\in values(a)} \n",
    "\\frac{|X_v|}{|X|} \\; entropy(X_v)\n",
    "$$\n",
    "\n",
    "where $X_v$ is the subset of $X$ for which ${a = v}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa4245a",
   "metadata": {},
   "source": [
    "### 7. Sigmoid Activation Function\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^\\ (-z)}\n",
    "$$\n",
    "with $z = w^Tx +b$ \\\n",
    "**Intuition**\n",
    "- Maps real values to probability range (0,1)  \n",
    "- Used in binary classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8444016f",
   "metadata": {},
   "source": [
    "### 8. Feature Scaling\n",
    "\n",
    "##### 8.1. Min-Max Scaling\n",
    "\n",
    "$$\n",
    "x_{scale} = \\frac{x- x_{\\min}}{x_{\\max}- x_{\\min}}\n",
    "$$\n",
    "**Where**\n",
    "- $x_{scale}$: original value  \n",
    "- $x_{\\min}, x_{\\max}$: feature min and max\n",
    "\n",
    "**Intuition**\n",
    "- Scales features to [0,1]  \n",
    "- Sensitive to outliers\n",
    "\n",
    "\n",
    "##### 8.2. Standardization (Mean Scaling)\n",
    "$$\n",
    "x' = \\frac{x- \\mu}{\\alpha}\n",
    "$$\n",
    "**Where**\n",
    "- $\\mu$: mean of feature  \n",
    "- $\\sigma$: standard deviation\n",
    "\n",
    "**Intuition**\n",
    "- Centers data at zero  \n",
    "- Makes gradient descent converge faster\n",
    "\n",
    "##### 8.3. Robust Scaling (Median Scaling)\n",
    "$$\n",
    "x' = \\frac{x - median(x)}{IQR}\n",
    "$$\n",
    "\n",
    "**Where**\n",
    "- \\(Q_1, Q_3\\): 25th and 75th percentiles\n",
    "- $IQR = Q3 -Q1$\n",
    "**Intuition**\n",
    "- Robust against outliers  \n",
    "- Useful for skewed data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3be873",
   "metadata": {},
   "source": [
    "### 9. Gradient Descent\n",
    "\n",
    "\n",
    "$$\n",
    "w := w - \\alpha \\frac{\\partial J(w)}{\\partial w}\n",
    "$$\n",
    "\n",
    "**Where**\n",
    "- $w$: model parameters  \n",
    "- $\\alpha$: learning rate  \n",
    "- $J(w)$: loss function\n",
    "\n",
    "**Intuition**\n",
    "- Updates weights in direction of steepest descent  \n",
    "- Goal is to minimize loss\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8911b1da",
   "metadata": {},
   "source": [
    "### 10. Stochastic Gradient Descent (SGD)\n",
    "\n",
    "$$\n",
    "w := w - \\alpha \\nabla J(w; x_i, y_i)\n",
    "$$\n",
    "\n",
    "**Where**\n",
    "- $x_i, y_i$: single training sample  \n",
    "- $\\nabla J$: gradient of loss\n",
    "\n",
    "**Intuition**\n",
    "- Faster updates  \n",
    "- More noisy but scalable\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be62593",
   "metadata": {},
   "source": [
    "### 11. MiniBatch SGD with Inertia (Momentum)\n",
    "$$\n",
    "v_t = \\beta v_{t-1} + \\nabla J(w)\n",
    "$$\n",
    "\n",
    "$w= w - \\alpha v_t $\n",
    "\n",
    "**Where**\n",
    "- $v_t$: velocity (momentum)  \n",
    "- $\\beta$: momentum coefficient (e.g. 0.9)\n",
    "\n",
    "**Intuition**\n",
    "- Accumulates past gradients  \n",
    "- Reduces oscillations  \n",
    "- Faster convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e7f850",
   "metadata": {},
   "source": [
    "### 12. The **R²(Coefficient of Determination)** indicates how much variance in the target is explained by the model.\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{SS_{\\text{res}}}{SS_{\\text{tot}}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $SS_{\\text{res}} = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$\n",
    "  (Residual Sum of Squares: error between true and predicted values)\n",
    "\n",
    "* $SS_{\\text{tot}} = \\sum_{i=1}^{n}(y_i - \\bar{y})^2$\n",
    "  (Total Sum of Squares: error between true values and their mean)\n",
    "\n",
    "* $y_i$: true value\n",
    "\n",
    "* $\\hat{y}_i$: predicted value\n",
    "\n",
    "* $\\bar{y}$: mean of all $y_i$\n",
    "\n",
    "Interpretation\n",
    "* $R^2 = 1$: perfect prediction\n",
    "* $R^2 = 0$: model predicts no better than mean\n",
    "* $R^2 < 0$: model is worse than just predicting the mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e96699",
   "metadata": {},
   "source": [
    "### 13. Euclidean distance\n",
    "$$\n",
    "{||x-c||}^2 = ({x_1} - {c_1})^2 + ({x_2} - {c_2})^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bba08e",
   "metadata": {},
   "source": [
    "### 14. Linear Regression\n",
    "\n",
    "$$\n",
    "y = b + \\sum_{i} {w_i}{x_i}\n",
    "$$\n",
    "\n",
    "Where: \\\n",
    "y: output \\\n",
    "${x_i}$: $i^{th}$ input \\\n",
    "${w_i}$: weight on $i^{th}$ input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3487678e",
   "metadata": {},
   "source": [
    "### 15. Lost Function for One Data Point\n",
    "\n",
    "\n",
    "$$\n",
    "(y^{<i>}- \\hat{y}^{<i>})^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7710fe00",
   "metadata": {},
   "source": [
    "### 16. Lasso Regression (L1)\n",
    "Lasso(L1 Regularization) minimizes the sum of squared errors **plus** a penalty proportional to the **absolute values of the coefficients**. This L1 penalty encourages **sparsity**, meaning it pushes some coefficients to **zero**, effectively performing feature selection.\n",
    "\n",
    "The loss function of Lasso regression is:\n",
    "\n",
    "$$\\mathcal{L}(\\beta) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( y_i - \\hat{y}_i \\right)^2 + \\alpha \\sum_{j=1}^{p} |w_j|$$\n",
    "\n",
    "Where:\n",
    "* $m$: number of samples\n",
    "* $y_i$: actual target value for sample $i$\n",
    "* $\\hat{y}_i = X_i \\cdot \\beta$: predicted value for sample $i$\n",
    "* $w_j$: the $j$-th coefficient of the model\n",
    "* $\\alpha \\geq 0$: regularization strength\n",
    "* The **first term** is the **mean squared error (MSE)**\n",
    "* The **second term** is the **L1 penalty** (sum of absolute values of the coefficients)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
